! Copyright (c) 2024 The YAC Authors
!
! SPDX-License-Identifier: BSD-3-Clause

! Main test program
PROGRAM access_core

  USE, INTRINSIC :: iso_c_binding
  USE mpi
  USE yac_core
  USE yac_utils

  IMPLICIT NONE

  ! file and grid names for reading input files
  CHARACTER(len=*), PARAMETER :: grid_filename = 'grids.nc'
  CHARACTER(len=*), PARAMETER :: mask_filename = 'masks_no_atm.nc'
  CHARACTER(len=*), PARAMETER :: src_grid_name = 'icos'
  CHARACTER(len=*), PARAMETER :: tgt_grid_name = 'ssea'
  LOGICAL, PARAMETER          :: src_use_ll = .FALSE.
  LOGICAL, PARAMETER          :: tgt_use_ll = .TRUE.

  ! data structures generated by YAC
  TYPE(c_ptr) :: src_grid, tgt_grid
  TYPE(c_ptr) :: dist_grid_pair
  TYPE(c_ptr) :: interp_grid
  TYPE(c_ptr) :: interp_stack_config
  TYPE(c_ptr) :: interp_method_stack
  TYPE(c_ptr) :: interp_weights

  ! information on duplicated cells
  TYPE(c_ptr) :: tgt_duplicated_cell_idx
  TYPE(c_ptr) :: tgt_orig_cell_global_id
  INTEGER(kind=c_size_t) :: tgt_nbr_duplicated_cells

  INTEGER :: comm_rank, comm_size, ierror

  ! initialize MPI and YAXT
  CALL yac_mpi_init_c()
  CALL yac_yaxt_init_c(MPI_COMM_WORLD)
  CALL mpi_comm_rank(MPI_COMM_WORLD, comm_rank, ierror)
  CALL mpi_comm_size(MPI_COMM_WORLD, comm_size, ierror)

  ! generate a basic grid
  !   In this example the root process reads in source grid and
  !   the last process reads in target data and they are the only
  !   ones to generate basic grids from their data.

  ! get the grid data
  src_grid = &
    read_basic_grid( &
      grid_filename, mask_filename, src_grid_name, src_use_ll)

  ! get the grid data
  tgt_grid = &
    read_basic_grid( &
      grid_filename, mask_filename, tgt_grid_name, tgt_use_ll, &
      tgt_duplicated_cell_idx, tgt_orig_cell_global_id, &
      tgt_nbr_duplicated_cells)

  ! generate distributed grid pair
  !   Generates an internal decomposition and redistributes the
  !   source and target grids accordingly. The decomposition
  !   is generated such that the grids are evenly distributed
  !   across all processes in the provided communicator.
  !   The source and target grid can be distributed in ANY
  !   decomposition, as long as the data is consistent and
  !   complete. (possible decompositions: one process provides
  !   everything; one process provides the source and another
  !   the target grid; one set of processes each provide a
  !   part of the source grid, while another set provides the
  !   target grid; all processes provide everything;...)
  !   All following operation involve all processes in the provided
  !   communicator (can be any communicator).
  !   (this operation is collective)
  dist_grid_pair = &
    yac_dist_grid_pair_new_c(src_grid, tgt_grid, MPI_COMM_WORLD)

  ! generate interpolation grid
  !   The interpolation grid contains a grid pair and assigns
  !   the source and target role to them. In addition, it contains
  !   additional information about the source and target fields
  !   (which coordinates and masks register in the basic grid
  !   is to be used).
  !   (this operation is collective)
  interp_grid =                                                      &
    yac_interp_grid_new_c(                                           &
      dist_grid_pair, TRIM(src_grid_name) // c_null_char,            &
      TRIM(tgt_grid_name) // c_null_char,  INT(1, c_size_t),         &
      (/INT(YAC_LOC_CELL, c_int)/), (/0_c_size_t/), (/-1_c_size_t/), &
      INT(YAC_LOC_CELL, c_int), 0_c_size_t, -1_c_size_t)

  ! configure the interpolation stack
  !   The interpolation stack configuration contains the information
  !   about the interpolation methods that are to be used in the
  !   interpolation. YAC has yac_interp_stack_config_add_* routines
  !   for all supported interpolation methods. In the weight
  !   computation, YAC starts with the first entry in the stack.
  !   All target points not interpolated by it, will be passed to
  !   the next method and so on...
  !   (the configuration has to be consistent on all processes)
  interp_stack_config = yac_interp_stack_config_new_c()
  CALL yac_interp_stack_config_add_conservative_c( &
    interp_stack_config, 1, 0, 1, YAC_INTERP_CONSERV_DESTAREA)
  CALL yac_interp_stack_config_add_nnn_c( &
    interp_stack_config, YAC_INTERP_NNN_AVG, INT(1, c_size_t), &
    YAC_INTERP_NNN_MAX_SEARCH_DISTANCE_DEFAULT_F, -1.0_c_double)

  ! generate the actual interpolation stack
  interp_method_stack = &
    yac_interp_stack_config_generate_c(interp_stack_config)

  ! execute the interpolation stack and generate the weights
  !   YAC starts by extracting all non-masked target points, which
  !   are then passed to the interpolation stack.
  !   The resulting interpolation weights contains the interpolation
  !   stencils, which are distributed across all processes.
  !   (this operation is collective)
  interp_weights = &
    yac_interp_method_do_search_c(interp_method_stack, interp_grid)

  CALL yac_duplicate_stencils_c( &
    interp_weights, tgt_grid, tgt_orig_cell_global_id, &
    tgt_duplicated_cell_idx, tgt_nbr_duplicated_cells, YAC_LOC_CELL)

  CALL yac_free_c(tgt_orig_cell_global_id)
  CALL yac_free_c(tgt_duplicated_cell_idx)

  ! optional: write weights to file
  !   In case you want to investigate the weights for example with
  !   the weights2vtk tool, you can write them into a weight file.
  !   This is done in parallel by a subset of all processes.
  CALL yac_interp_weights_write_to_file_c(             &
    interp_weights, TRIM("weights.nc") // c_null_char, &
    TRIM(src_grid_name) // c_null_char,                &
    TRIM(tgt_grid_name) // c_null_char,                &
    get_global_grid_size(src_grid),                    &
    get_global_grid_size(tgt_grid))

  ! cleanup
  CALL yac_interp_weights_delete_c(interp_weights)
  CALL yac_interp_method_delete_c(interp_method_stack)
  CALL yac_free_c(interp_method_stack)
  CALL yac_interp_stack_config_delete_c(interp_stack_config)
  CALL yac_interp_grid_delete_c(interp_grid)
  CALL yac_dist_grid_pair_delete_c(dist_grid_pair)
  CALL yac_basic_grid_delete_c(tgt_grid)
  CALL yac_basic_grid_delete_c(src_grid)

  ! finalize MPI and YAXT
  CALL yac_mpi_finalize_c()

CONTAINS

  FUNCTION itoa(i) result(res)
    CHARACTER(:),ALLOCATABLE :: res
    INTEGER,INTENT(in) :: i
    CHARACTER(RANGE(i)+2) :: tmp
    WRITE(tmp,'(i0)') i
    res = trim(tmp)
  END FUNCTION

  FUNCTION read_basic_grid( &
    grid_filename, mask_filename, &
    grid_name, use_ll, duplicated_cell_idx, &
    orig_cell_global_id, nbr_duplicated_cells)

    USE, INTRINSIC :: iso_c_binding
    USE yac_core

    CHARACTER(len=*), INTENT(IN) :: grid_filename
    CHARACTER(len=*), INTENT(IN) :: mask_filename
    CHARACTER(len=*), INTENT(IN) :: grid_name
    LOGICAL, INTENT(IN)          :: use_ll

    TYPE(c_ptr), OPTIONAL, INTENT(OUT) :: duplicated_cell_idx
    TYPE(c_ptr), OPTIONAL, INTENT(OUT) :: orig_cell_global_id
    INTEGER(kind=c_size_t), OPTIONAL, INTENT(OUT) :: nbr_duplicated_cells

    TYPE(c_ptr) :: read_basic_grid

    TYPE(c_ptr) :: duplicated_cell_idx_
    TYPE(c_ptr) :: orig_cell_global_id_
    INTEGER(kind=c_size_t) :: nbr_duplicated_cells_
    INTEGER(kind=c_size_t) :: coordinates_idx

!AP account for grid type. FIND A WAY TO MAKE IT KNOW IN OASIS via the namcouple
!AP DO WE NEED A REGULAR CONSTRUCTOR (e.g. for bggd) ?

    duplicated_cell_idx_ = c_null_ptr
    orig_cell_global_id_ = c_null_ptr
    nbr_duplicated_cells_ = 0

    ! read in grid data from SCRIP formated files
    read_basic_grid = &
      yac_read_scrip_basic_grid_parallel_c( &
        TRIM(grid_filename) // c_null_char, &
        TRIM(mask_filename) // c_null_char, &
        MPI_COMM_WORLD, TRIM(grid_name) // c_null_char, &
        INT(0, c_int), TRIM(grid_name) // c_null_char, &
        INT(MERGE(1,0, use_ll), c_int), coordinates_idx, &
        MERGE( &
          duplicated_cell_idx_, c_null_ptr, PRESENT(duplicated_cell_idx)), &
        MERGE( &
          orig_cell_global_id_, c_null_ptr, PRESENT(orig_cell_global_id)), &
        nbr_duplicated_cells_)

    IF (PRESENT(duplicated_cell_idx)) &
      duplicated_cell_idx = duplicated_cell_idx_
    IF (PRESENT(orig_cell_global_id)) &
      orig_cell_global_id = orig_cell_global_id_
    IF (PRESENT(nbr_duplicated_cells)) &
      nbr_duplicated_cells = nbr_duplicated_cells_

    ! DEBUG: write out grid data
    CALL yac_write_basic_grid_to_file_c( &
      read_basic_grid, &
      TRIM(grid_name) // '_' // itoa(comm_rank) // c_null_char)

  END FUNCTION read_basic_grid

  FUNCTION get_global_grid_size(grid)

    USE, INTRINSIC :: iso_c_binding
    USE yac_core

    TYPE(c_ptr), INTENT(IN) :: grid

    INTEGER(kind=c_size_t) :: get_global_grid_size

    INTEGER :: local_grid_size(1), global_grid_size(1), ierror

    local_grid_size(1) = &
        INT(yac_basic_grid_get_data_size_c(grid, YAC_LOC_CELL))
    CALL MPI_Allreduce( &
        local_grid_size, global_grid_size, 1, MPI_INTEGER, &
        MPI_SUM, MPI_COMM_WORLD, ierror)
    get_global_grid_size = INT(global_grid_size(1), c_size_t)

  END FUNCTION get_global_grid_size

END PROGRAM access_core
